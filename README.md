#  SystÃ¨me de Surveillance du Trafic Routier en Temps RÃ©el

**Plateforme Big Data pour l'analyse et la visualisation du trafic routier de Paris en temps rÃ©el**

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![React](https://img.shields.io/badge/React-18-61DAFB.svg)](https://reactjs.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104-009688.svg)](https://fastapi.tiangolo.com/)
[![Apache Spark](https://img.shields.io/badge/Spark-3.5-E25A1C.svg)](https://spark.apache.org/)
[![Apache Kafka](https://img.shields.io/badge/Kafka-7.5-231F20.svg)](https://kafka.apache.org/)
[![HDFS](https://img.shields.io/badge/HDFS-3.2-FF6F00.svg)](https://hadoop.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-336791.svg)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED.svg)](https://www.docker.com/)

---

## ğŸ¯ Vue d'Ensemble

Ce projet est une **plateforme complÃ¨te de surveillance du trafic routier** qui combine technologies Big Data et temps rÃ©el pour offrir une vision exhaustive du trafic parisien.

### Architecture Lambda

Le systÃ¨me implÃ©mente une **architecture Lambda** combinant :
- **Batch Layer** (HDFS) : Historique fiable et immuable
- **Speed Layer** (Kafka + Spark Streaming) : DonnÃ©es temps rÃ©el
- **Serving Layer** (FastAPI + PostgreSQL) : API unifiÃ©e

### Cas d'Usage

- **Monitoring temps rÃ©el** : Surveillance de 5+ zones avec mise Ã  jour toutes les 60 secondes
- **Cartographie interactive** : Visualisation gÃ©ographique dynamique
- **Tableaux de bord** : KPIs et mÃ©triques opÃ©rationnelles
- **Analyse historique** : Tendances sur plusieurs mois/annÃ©es (30 jours â†’ illimitÃ©)

---

## FonctionnalitÃ©s

### Frontend

- **Dashboard temps rÃ©el** avec WebSocket (mise Ã  jour automatique toutes les 5s)
- **Carte interactive** Leaflet avec marqueurs colorÃ©s dynamiques
- **Graphiques analytiques** Recharts :
  - Ã‰volution hebdomadaire (graphique en aire)
  - Distribution horaire (graphique en barres)
  - RÃ©partition par zone (graphique circulaire)
  - Performance systÃ¨me (radar chart)
- **Filtres temporels** : 24h, 7 jours, 30 jours, 1 an
- **Responsive design** adaptatif mobile/desktop (Tailwind CSS)
- **Animations fluides** et transitions (Framer Motion)
- **Dark mode ready** (architecture prÃ©parÃ©e)

### Backend

- **API REST** FastAPI avec documentation auto-gÃ©nÃ©rÃ©e (Swagger/OpenAPI)
- **WebSocket** bidirectionnel pour push temps rÃ©el
- **IntÃ©gration TomTom API** avec retry et fallback
- **ORM SQLAlchemy** avec migrations Alembic
- **Lecture HDFS distribuÃ©e** via PySpark
- **Gestion d'erreurs** complÃ¨te avec logging structurÃ©
- **Validation Pydantic** des donnÃ©es entrantes/sortantes
- **CORS configurÃ©** pour dÃ©veloppement et production

### Big Data

- **Kafka** : Ingestion haute performance (10,000+ msg/s)
- **Spark Streaming** : Traitement micro-batches (30s)
- **HDFS** : Stockage distribuÃ© avec partitionnement par date
- **Format Parquet** : Compression 80% + lecture colonne optimisÃ©e
- **Checkpointing** : Exactly-once semantics (pas de perte/duplication)
- **RÃ©silience** : Retry automatique, circuit breaker pattern

---

## Architecture

### SchÃ©ma d'Architecture Complet
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SOURCES DE DONNÃ‰ES                          â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   TomTom API     â”‚              â”‚   GÃ©nÃ©rateur    â”‚         â”‚
â”‚  â”‚ (DonnÃ©es RÃ©elles)â”‚              â”‚  (Simulation)   â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚           â”‚                                 â”‚                   â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                        â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INGESTION LAYER                               â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Producer Kafka (Python)                                  â”‚  â”‚
â”‚  â”‚  - Collecte donnÃ©es toutes les 60s                        â”‚  â”‚
â”‚  â”‚  - Validation et enrichissement                           â”‚  â”‚
â”‚  â”‚  - Retry automatique (5 tentatives)                       â”‚  â”‚
â”‚  â”‚  - Compression gzip                                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MESSAGE BROKER                                â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Apache Kafka + Zookeeper                                 â”‚  â”‚
â”‚  â”‚  Topic: traffic_raw                                       â”‚  â”‚
â”‚  â”‚  - Partitions: 1                                          â”‚  â”‚
â”‚  â”‚  - Replication: 1                                         â”‚  â”‚
â”‚  â”‚  - Retention: 7 jours (rejouable)                        â”‚  â”‚
â”‚  â”‚  - Format: JSON compressÃ© (gzip)                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  STREAM PROCESSING LAYER                         â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Apache Spark Streaming                                   â”‚  â”‚
â”‚  â”‚  - Lecture Kafka (micro-batch 30s)                       â”‚  â”‚
â”‚  â”‚  - Nettoyage (filtres, validation)                       â”‚  â”‚
â”‚  â”‚  - Transformation (calculs, enrichissement)              â”‚  â”‚
â”‚  â”‚  - AgrÃ©gation (moyennes, comptages)                      â”‚  â”‚
â”‚  â”‚  - Checkpointing (exactly-once)                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                  â”‚                          â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚    â”‚                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL     â”‚  â”‚       HDFS         â”‚  â”‚    Backend        â”‚
â”‚   (Hot Data)     â”‚  â”‚   (Cold Data)      â”‚  â”‚   Cache Ready     â”‚
â”‚                  â”‚  â”‚                    â”‚  â”‚                   â”‚
â”‚ â€¢ < 1 heure      â”‚  â”‚ â€¢ Historique âˆ     â”‚  â”‚ â€¢ Redis (future)  â”‚
â”‚ â€¢ Index B-Tree   â”‚  â”‚ â€¢ Format Parquet   â”‚  â”‚                   â”‚
â”‚ â€¢ Pool 10+20     â”‚  â”‚ â€¢ Compression 80%  â”‚  â”‚                   â”‚
â”‚ â€¢ RequÃªtes <10ms â”‚  â”‚ â€¢ PartitionnÃ©      â”‚  â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                     â”‚                        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SERVING LAYER                             â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  FastAPI Backend                                          â”‚  â”‚
â”‚  â”‚  - REST API (sync + async)                               â”‚  â”‚
â”‚  â”‚  - WebSocket (temps rÃ©el)                                â”‚  â”‚
â”‚  â”‚  - SQLAlchemy ORM                                        â”‚  â”‚
â”‚  â”‚  - PySpark pour HDFS                                     â”‚  â”‚
â”‚  â”‚  - Swagger UI (/docs)                                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PRESENTATION LAYER                             â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  React Frontend (Vite)                                    â”‚  â”‚
â”‚  â”‚  - Dashboard temps rÃ©el (WebSocket)                       â”‚  â”‚
â”‚  â”‚  - Carte interactive (Leaflet)                            â”‚  â”‚
â”‚  â”‚  - Analytics (Recharts + HDFS)                           â”‚  â”‚
â”‚  â”‚  - Responsive (Tailwind CSS)                             â”‚  â”‚
â”‚  â”‚  - Animations (Framer Motion)                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flux de DonnÃ©es DÃ©taillÃ©
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1 : INGESTION (t=0s)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
TomTom API (HTTPS GET)
    â†“ JSON Response
Producer Python (tomtom_producer_fixed.py)
    â†“ Validation + Enrichissement
Kafka Topic "traffic_raw"
    â†“ Persistance 7 jours

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2 : TRANSFORMATION (t=0-30s)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Spark Streaming (streaming_docker.py)
    â”œâ”€ Lecture Kafka (micro-batch)
    â”œâ”€ Parsing JSON â†’ DataFrame
    â”œâ”€ Nettoyage (valeurs aberrantes)
    â”œâ”€ Calcul congestion_level
    â”œâ”€ Enrichissement (year, month, day, hour)
    â””â”€ Validation finale

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3 : STOCKAGE (t=30s)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Spark Write
    â”œâ”€ PostgreSQL (JDBC)
    â”‚   â””â”€ INSERT INTO traffic_data
    â”‚       - Index automatique
    â”‚       - DurÃ©e: < 1 heure
    â”‚
    â””â”€ HDFS (Parquet)
        â””â”€ APPEND /traffic/clean/year=2025/month=11/day=21/
            - Compression 80%
            - DurÃ©e: âˆ

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4 : SERVING (t=30s â†’ âˆ)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
FastAPI Backend
    â”œâ”€ GET /api/zones/live
    â”‚   â””â”€ PostgreSQL (< 10ms)
    â”‚
    â”œâ”€ GET /api/zones/weekly-data?days=7
    â”‚   â””â”€ HDFS via Spark (2-5s)
    â”‚       - Partition pruning
    â”‚       - Predicate pushdown
    â”‚
    â””â”€ WebSocket /ws/traffic
        â””â”€ Push automatique (5s interval)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 5 : VISUALISATION (t=30s â†’ âˆ)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
React Frontend
    â”œâ”€ Dashboard
    â”‚   â””â”€ WebSocket â†’ Mise Ã  jour auto
    â”‚
    â”œâ”€ Carte
    â”‚   â””â”€ Leaflet â†’ Marqueurs dynamiques
    â”‚
    â””â”€ Analytics
        â””â”€ Recharts â†’ Graphiques HDFS
```

---

## Technologies

### Stack Backend

| Technologie | Version | RÃ´le | Documentation |
|-------------|---------|------|---------------|
| **Python** | 3.11 | Langage principal | [python.org](https://www.python.org/) |
| **FastAPI** | 0.104+ | Framework API REST | [fastapi.tiangolo.com](https://fastapi.tiangolo.com/) |
| **SQLAlchemy** | 2.0+ | ORM PostgreSQL | [sqlalchemy.org](https://www.sqlalchemy.org/) |
| **PySpark** | 3.5.0 | Traitement Big Data | [spark.apache.org](https://spark.apache.org/) |
| **Uvicorn** | 0.24+ | Serveur ASGI | [uvicorn.org](https://www.uvicorn.org/) |
| **Pydantic** | 2.0+ | Validation donnÃ©es | [pydantic.dev](https://docs.pydantic.dev/) |
| **kafka-python** | 2.0+ | Client Kafka | [pypi.org/kafka-python](https://pypi.org/project/kafka-python/) |

### Stack Frontend

| Technologie | Version | RÃ´le | Documentation |
|-------------|---------|------|---------------|
| **React** | 18.2+ | Framework UI | [react.dev](https://react.dev/) |
| **Vite** | 5.0+ | Build tool | [vitejs.dev](https://vitejs.dev/) |
| **Tailwind CSS** | 3.4+ | Framework CSS | [tailwindcss.com](https://tailwindcss.com/) |
| **React Leaflet** | 4.2+ | Cartographie | [react-leaflet.js.org](https://react-leaflet.js.org/) |
| **Recharts** | 2.10+ | Graphiques | [recharts.org](https://recharts.org/) |
| **Framer Motion** | 10.16+ | Animations | [framer.com/motion](https://www.framer.com/motion/) |
| **Lucide React** | 0.263+ | IcÃ´nes | [lucide.dev](https://lucide.dev/) |

### Stack Big Data

| Technologie | Version | RÃ´le | Documentation |
|-------------|---------|------|---------------|
| **Apache Kafka** | 7.5.0 | Message broker | [kafka.apache.org](https://kafka.apache.org/) |
| **Apache Spark** | 3.5.0 | Stream processing | [spark.apache.org](https://spark.apache.org/) |
| **Hadoop HDFS** | 3.2.1 | Stockage distribuÃ© | [hadoop.apache.org](https://hadoop.apache.org/) |
| **PostgreSQL** | 15 | Base temps rÃ©el | [postgresql.org](https://www.postgresql.org/) |
| **Zookeeper** | 7.5.0 | Coordination | [zookeeper.apache.org](https://zookeeper.apache.org/) |

### DevOps

| Technologie | Version | RÃ´le | Documentation |
|-------------|---------|------|---------------|
| **Docker** | 24+ | Conteneurisation | [docker.com](https://www.docker.com/) |
| **Docker Compose** | 2.23+ | Orchestration | [docs.docker.com](https://docs.docker.com/compose/) |

---

## PrÃ©requis

### Configuration SystÃ¨me

| Ressource | Minimum | RecommandÃ© | Optimal |
|-----------|---------|------------|---------|
| **RAM** | 8 GB | 16 GB | 32 GB |
| **CPU** | 4 cores | 8 cores | 16 cores |
| **Disque** | 20 GB | 50 GB | 100 GB |
| **OS** | Linux/macOS/Windows (WSL2) | Linux | Linux |

### Logiciels Requis
```bash
# Docker & Docker Compose
docker --version
# Docker version 24.0.0 ou supÃ©rieur

docker-compose --version
# Docker Compose version 2.23.0 ou supÃ©rieur

# Python (pour dÃ©veloppement local)
python --version
# Python 3.11 ou supÃ©rieur

# Node.js (pour dÃ©veloppement frontend)
node --version
# Node v18 ou supÃ©rieur

npm --version
# npm 9 ou supÃ©rieur
```

### VÃ©rification des Ports

Les ports suivants doivent Ãªtre disponibles :

| Port | Service | Description |
|------|---------|-------------|
| 5173 | Frontend | Interface React |
| 8000 | Backend | API FastAPI |
| 5432 | PostgreSQL | Base de donnÃ©es |
| 9092 | Kafka | Message broker |
| 2181 | Zookeeper | Coordination |
| 9000 | HDFS NameNode | RPC |
| 9870 | HDFS NameNode | WebUI |
| 9864 | HDFS DataNode | WebUI |
```bash
# VÃ©rifier qu'un port est libre
lsof -i :5173
# Si rien ne s'affiche â†’ Port libre 
```

### ClÃ© API TomTom (Optionnel)

Pour utiliser des donnÃ©es de trafic rÃ©elles :

1. **CrÃ©er un compte** : [developer.tomtom.com](https://developer.tomtom.com/)
2. **Obtenir une clÃ© API** : Plan gratuit disponible (2500 requÃªtes/jour)
3. **Configurer** : Ajouter dans `.env`

---

## Installation

### Installation Rapide (5 minutes)
```bash
# 1. Cloner le projet
git clone https://github.com/votre-username/traffic-monitoring-system.git
cd traffic-monitoring-system

# 2. CrÃ©er hadoop.env
cat > hadoop.env << 'EOF'
CORE_CONF_fs_defaultFS=hdfs://namenode:9000
CORE_CONF_hadoop_http_staticuser_user=root
HDFS_CONF_dfs_webhdfs_enabled=true
HDFS_CONF_dfs_permissions_enabled=false
HDFS_CONF_dfs_replication=1
EOF

# 3. Lancer l'infrastructure
docker-compose up -d

# 4. Attendre l'initialisation
sleep 60

# 5. Charger les donnÃ©es historiques
docker-compose exec backend python3 csv_to_hdfs_docker.py --days 30

# 6. Ouvrir le frontend
open http://localhost:5173
```

### Installation DÃ©taillÃ©e

#### Ã‰tape 1 : Cloner le Projet
```bash
git clone https://github.com/votre-username/traffic-monitoring-system.git
cd traffic-monitoring-system
```

#### Ã‰tape 2 : Configuration

##### CrÃ©er `hadoop.env`
```bash
cat > hadoop.env << 'EOF'
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Configuration Hadoop HDFS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Core Configuration
CORE_CONF_fs_defaultFS=hdfs://namenode:9000
CORE_CONF_hadoop_http_staticuser_user=root

# HDFS Configuration
HDFS_CONF_dfs_webhdfs_enabled=true
HDFS_CONF_dfs_permissions_enabled=false
HDFS_CONF_dfs_replication=1
HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false

# Performance
HDFS_CONF_dfs_datanode_max_transfer_threads=8192
HDFS_CONF_dfs_datanode_max_xcievers=8192
HDFS_CONF_dfs_datanode_disk_check_min_gap=30s
EOF
```

##### CrÃ©er `.env` (Optionnel)
```bash
cat > .env << 'EOF'
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Configuration Environnement
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# TomTom API (optionnel)
TOMTOM_API_KEY=votre_clÃ©_api_ici

# PostgreSQL
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=traffic

# Kafka
KAFKA_BOOTSTRAP_SERVERS=kafka:9092

# Backend
DATABASE_URL=postgresql://postgres:postgres@postgres:5432/traffic
DOCKER_ENV=true
EOF
```

#### Ã‰tape 3 : DÃ©marrer les Services
```bash
# DÃ©marrer tous les conteneurs
docker-compose up -d

# VÃ©rifier le statut
docker-compose ps

# RÃ©sultat attendu :
# NAME        IMAGE                                       STATUS
# namenode    bde2020/hadoop-namenode:2.0.0-hadoop3...    Up (healthy)
# datanode    bde2020/hadoop-datanode:2.0.0-hadoop3...    Up
# kafka       confluentinc/cp-kafka:7.5.0                Up (healthy)
# zookeeper   confluentinc/cp-zookeeper:7.5.0            Up
# postgres    postgres:15-alpine                         Up (healthy)
# backend     traffic-backend                            Up
# frontend    traffic-frontend                           Up
```

#### Ã‰tape 4 : VÃ©rifier l'Initialisation
```bash
# VÃ©rifier les logs
docker-compose logs backend | tail -20

# Devrait afficher :
# Tables crÃ©Ã©es dans PostgreSQL
# Zones insÃ©rÃ©es : 5 zones
# DÃ©marrage du backend...
# Initialisation HDFS Service - Host: hdfs://namenode:9000
# HDFS Service initialisÃ© - Path: /traffic
# INFO: Started server process
# INFO: Waiting for application startup.
# INFO: Application startup complete.
# INFO: Uvicorn running on http://0.0.0.0:8000
```
```bash
# VÃ©rifier HDFS
docker exec namenode hadoop fs -ls /

# RÃ©sultat : Found 0 items (normal au premier lancement)
```
```bash
# VÃ©rifier PostgreSQL
docker exec postgres psql -U postgres -d traffic -c "\dt"

# Devrait afficher :
#           List of relations
#  Schema |     Name      | Type  |  Owner   
# --------+---------------+-------+----------
#  public | traffic_data  | table | postgres
#  public | zones         | table | postgres
```

#### Ã‰tape 5 : Charger les DonnÃ©es Historiques
```bash
# Entrer dans le conteneur backend
docker-compose exec backend bash

# GÃ©nÃ©rer 30 jours de donnÃ©es simulÃ©es
python3 csv_to_hdfs_docker.py --days 30

# RÃ©sultat attendu :
# Spark Session crÃ©Ã©e (Docker) 
# GÃ©nÃ©ration donnÃ©es historiques (30 jours)...
# 3600 lignes gÃ©nÃ©rÃ©es
# Ã‰criture dans HDFS (partitionnÃ©)...
# DonnÃ©es Ã©crites dans: /traffic/clean
# CrÃ©ation agrÃ©gats horaires...
# AgrÃ©gats Ã©crits dans: /traffic/aggregates/hourly
# Import terminÃ© avec succÃ¨s!

# Sortir du conteneur
exit
```

#### Ã‰tape 6 : Lancer le Producer (Optionnel)
```bash
# Option A : Avec TomTom API (si vous avez une clÃ©)
cd spark
python tomtom_producer_fixed.py --api-key VOTRE_CLE --continuous --interval 60

# Option B : Mode simulation (sans clÃ© API)
# Vous pouvez crÃ©er un gÃ©nÃ©rateur simple ou utiliser uniquement les donnÃ©es HDFS
```

#### Ã‰tape 7 : AccÃ©der aux Interfaces

| Service | URL | Identifiants |
|---------|-----|--------------|
| **Frontend Principal** | http://localhost:5173 | Aucun |
| **API Backend** | http://localhost:8000 | Aucun |
| **Documentation API** | http://localhost:8000/docs | Aucun |
| **HDFS NameNode UI** | http://localhost:9870 | Aucun |
| **PostgreSQL** | localhost:5432 | postgres/postgres |

---

## Utilisation

### Interface Web

#### 1. Dashboard Principal

AccÃ©dez Ã  **http://localhost:5173**

**Composants visibles :**
- **KPIs temps rÃ©el** : Congestion moyenne, nombre de vÃ©hicules, zones actives
- **Carte interactive** : Zones de trafic avec marqueurs colorÃ©s (vert/orange/rouge)
- **Liste des zones** : DÃ©tails par zone avec statut temps rÃ©el
- **Mise Ã  jour automatique** : Via WebSocket toutes les 5 secondes

**Actions disponibles :**
```
- Clic sur marqueur â†’ Popup avec dÃ©tails
- Zoom molette â†’ Zoom carte
- Navigation â†’ Menu latÃ©ral (Dashboard / Analytics / Settings)
```

#### 2. Page Analytics

AccÃ©dez Ã  **http://localhost:5173** â†’ Clic sur **"Analyses"**

**Graphiques disponibles :**

**Ã‰volution Hebdomadaire**
```
- Type : Graphique en aire (Area Chart)
- Axe X : Jours de la semaine (Lun, Mar, Mer, ...)
- Axe Y : Congestion moyenne (%)
- Filtres : 24h, 7j, 30j, 1 an
- Source : HDFS (via Spark SQL)
```

**Distribution Horaire**
```
- Type : Graphique en barres (Bar Chart)
- Axe X : Heures de la journÃ©e (00h, 03h, 06h, ...)
- Axe Y : Trafic moyen (%)
- Couleurs : Vert (<40%), Orange (40-70%), Rouge (>70%)
- Source : HDFS (agrÃ©gats horaires)
```

**RÃ©partition par Zone**
```
- Type : Graphique circulaire (Pie Chart)
- Affichage : Pourcentage par zone
- LÃ©gende : Nom des zones
```

**Filtrage temporel :**
```bash
# SÃ©lecteur en haut Ã  droite
[DerniÃ¨res 24h] [7 derniers jours] [30 derniers jours] [Cette annÃ©e]

# Comportement :
# - Changement â†’ Rechargement automatique des donnÃ©es
# - Loader â†’ Indicateur de chargement
# - Badge HDFS â†’ Confirmation source de donnÃ©es
```

#### 3. Page Settings (Ã€ venir)

AccÃ©dez Ã  **http://localhost:5173** â†’ Clic sur **"ParamÃ¨tres"**

**FonctionnalitÃ©s prÃ©vues :**
- Configuration des zones
- Seuils d'alerte personnalisÃ©s
- PrÃ©fÃ©rences d'affichage
- Export de donnÃ©es

### API REST

#### Endpoints Principaux

##### 1. Zones en Temps RÃ©el
```bash
# RÃ©cupÃ©rer toutes les zones avec donnÃ©es temps rÃ©el
curl http://localhost:8000/api/zones/live

# RÃ©ponse :
[
  {
    "id": "champs_elysees",
    "name": "Champs-Ã‰lysÃ©es",
    "location": "Paris 8e",
    "latitude": 48.8698,
    "longitude": 2.3078,
    "current_speed": 45.5,
    "free_flow_speed": 60.0,
    "congestion_level": 24.17,
    "status": "Fluide",
    "vehicles": 234
  },
  ...
]
```

##### 2. DonnÃ©es Hebdomadaires (HDFS)
```bash
# RÃ©cupÃ©rer donnÃ©es sur 7 jours
curl "http://localhost:8000/api/zones/weekly-data?days=7"

# RÃ©ponse :
[
  {
    "date": "2025-11-15",
    "congestion": 45.2,
    "speed": 52.3,
    "measures": 120
  },
  {
    "date": "2025-11-16",
    "congestion": 52.8,
    "speed": 48.1,
    "measures": 120
  },
  ...
]
```

##### 3. Distribution Horaire (HDFS)
```bash
# RÃ©cupÃ©rer distribution sur 30 jours
curl "http://localhost:8000/api/zones/hourly-distribution?days=30"

# RÃ©ponse :
[
  {"hour": "00h", "congestion": 15.2},
  {"hour": "03h", "congestion": 8.5},
  {"hour": "06h", "congestion": 35.7},
  {"hour": "09h", "congestion": 85.4},
  ...
]
```

##### 4. Statistiques HDFS
```bash
# RÃ©cupÃ©rer infos sur le stockage HDFS
curl http://localhost:8000/api/zones/hdfs-stats

# RÃ©ponse :
{
  "available": true,
  "total_records": 3600,
  "earliest_record": "2025-10-21 08:40:25",
  "latest_record": "2025-11-20 08:40:25",
  "hdfs_path": "/traffic/clean"
}
```

##### 5. Statistiques AgrÃ©gÃ©es
```bash
# RÃ©cupÃ©rer agrÃ©gats temps rÃ©el
curl http://localhost:8000/api/aggregates/stats

# RÃ©ponse :
{
  "totalZones": 5,
  "activeZones": 5,
  "averageCongestion": 45.2,
  "totalVehicles": 1234,
  "criticalZones": 1
}
```


### WebSocket (Temps RÃ©el)

#### Connexion Frontend
```javascript
// Connexion WebSocket
const ws = new WebSocket('ws://localhost:8000/ws/traffic');

// Gestion des Ã©vÃ©nements
ws.onopen = () => {
    console.log(' WebSocket connectÃ©');
};

ws.onmessage = (event) => {
    const data = JSON.parse(event.data);
    console.log(' Mise Ã  jour reÃ§ue:', data);
    
    // Structure :
    // {
    //   "zones": [...],  // Liste des zones mises Ã  jour
    //   "stats": {...}   // Statistiques globales
    // }
    
    // Mettre Ã  jour l'UI
    updateZones(data.zones);
    updateStats(data.stats);
};

ws.onerror = (error) => {
    console.error(' Erreur WebSocket:', error);
};

ws.onclose = () => {
    console.log(' WebSocket dÃ©connectÃ©');
    // Reconnexion automatique aprÃ¨s 5s
    setTimeout(() => {
        reconnect();
    }, 5000);
};
```

#### Test Manuel (wscat)
```bash
# Installer wscat
npm install -g wscat

# Se connecter
wscat -c ws://localhost:8000/ws/traffic

# Vous devriez recevoir des messages toutes les 5 secondes :
< {"zones": [...], "stats": {...}}
< {"zones": [...], "stats": {...}}
< {"zones": [...], "stats": {...}}
```

---

## ğŸ—‚ï¸ Structure du Projet
```
traffic-monitoring-system/
â”‚
â”œâ”€â”€  backend/                         # Backend FastAPI
â”‚   â”œâ”€â”€  app/
â”‚   â”‚   â”œâ”€â”€  api/
â”‚   â”‚   â”‚   â””â”€â”€  routes/
â”‚   â”‚   â”‚       â”œâ”€â”€ zones.py          # Routes zones (live, weekly, HDFS)
â”‚   â”‚   â”‚       â””â”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€  services/
â”‚   â”‚   â”‚   â”œâ”€â”€ hdfs_service.py       # Service lecture HDFS
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ models.py                 # ModÃ¨les SQLAlchemy (Zone, TrafficData)
â”‚   â”‚   â”œâ”€â”€ database.py               # Configuration PostgreSQL
â”‚   â”‚   â”œâ”€â”€ init_db.py                # Initialisation tables
â”‚   â”‚   â”œâ”€â”€ main.py                   # Point d'entrÃ©e FastAPI
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ requirements.txt              # DÃ©pendances Python
â”‚   â”œâ”€â”€ Dockerfile                    # Image Docker backend
â”‚   â””â”€â”€ csv_to_hdfs_docker.py         # Script import HDFS
â”‚
â”œâ”€â”€  frontend/                        # Frontend React
â”‚   â”œâ”€â”€  src/
â”‚   â”‚   â”œâ”€â”€  components/
â”‚   â”‚   â”‚   â”œâ”€â”€  Map/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Map.jsx           # Carte Leaflet
â”‚   â”‚   â”‚   â”œâ”€â”€  Dashboard/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Dashboard.jsx     # Dashboard principal
â”‚   â”‚   â”‚   â”œâ”€â”€  Analytics/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Analytics.jsx     # Page analytics
â”‚   â”‚   â”‚   â””â”€â”€  Settings/
â”‚   â”‚   â”‚       â””â”€â”€ Settings.jsx      # ParamÃ¨tres
â”‚   â”‚   â”œâ”€â”€  services/
â”‚   â”‚   â”‚   â”œâ”€â”€ api.js                # Client API REST
â”‚   â”‚   â”‚   â””â”€â”€ websocket.js          # Client WebSocket
â”‚   â”‚   â”œâ”€â”€ App.jsx                   # Composant racine
â”‚   â”‚   â”œâ”€â”€ main.jsx                  # Point d'entrÃ©e
â”‚   â”‚   â””â”€â”€ index.css                 # Styles globaux
â”‚   â”œâ”€â”€ package.json                  # DÃ©pendances npm
â”‚   â”œâ”€â”€ vite.config.js                # Configuration Vite
â”‚   â”œâ”€â”€ tailwind.config.js            # Configuration Tailwind
â”‚   â”œâ”€â”€ Dockerfile                    # Image Docker frontend
â”‚   â””â”€â”€ index.html                    # Template HTML
â”‚
â”œâ”€â”€  spark/                           # Scripts Big Data
â”‚   â”œâ”€â”€ tomtom_producer.py            # Producer Kafka
â”‚   â”œâ”€â”€ csv_to_hdfs_docker.py         # Import batch HDFS
â”‚
â”œâ”€â”€ docker-compose.yml                # Orchestration services
â”œâ”€â”€ hadoop.env                        # Configuration Hadoop
â”œâ”€â”€ .env                              # Variables d'environnement
â”œâ”€â”€ .gitignore                        # Fichiers ignorÃ©s Git
â”œâ”€â”€ README.md                         # Ce fichier
```

---

##  Pipeline ETL

### Vue d'Ensemble ETL
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   EXTRACT (Extraction)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  SOURCE 1: TomTom API                                        â”‚
â”‚  â”œâ”€ GET https://api.tomtom.com/traffic/services/4/...      â”‚
â”‚  â”œâ”€ Format: JSON                                             â”‚
â”‚  â”œâ”€ FrÃ©quence: 60 secondes                                  â”‚
â”‚  â””â”€ Authentification: API Key                               â”‚
â”‚                                                              â”‚
â”‚  SOURCE 2: GÃ©nÃ©rateur Python (Simulation)                   â”‚
â”‚  â”œâ”€ Algorithme: Patterns horaires rÃ©alistes                â”‚
â”‚  â”œâ”€ Format: Dict Python â†’ JSON                              â”‚
â”‚  â””â”€ FrÃ©quence: Configurable                                 â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 TRANSFORM (Transformation)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Ã‰TAPE 1: Parsing                                            â”‚
â”‚  â””â”€ from_json(col("value"), schema)                         â”‚
â”‚                                                              â”‚
â”‚  Ã‰TAPE 2: Nettoyage                                          â”‚
â”‚  â”œâ”€ Filtre: current_speed > 0                               â”‚
â”‚  â”œâ”€ Filtre: current_speed <= 200                            â”‚
â”‚  â”œâ”€ Filtre: congestion_level >= 0                           â”‚
â”‚  â””â”€ Suppression: valeurs NULL                               â”‚
â”‚                                                              â”‚
â”‚  Ã‰TAPE 3: Calculs                                            â”‚
â”‚  â””â”€ congestion_level = (free_flow - current) / free_flow Ã— 100 â”‚
â”‚                                                              â”‚
â”‚  Ã‰TAPE 4: Enrichissement                                     â”‚
â”‚  â”œâ”€ Ajout: year = year(timestamp)                           â”‚
â”‚  â”œâ”€ Ajout: month = month(timestamp)                         â”‚
â”‚  â”œâ”€ Ajout: day = dayofmonth(timestamp)                      â”‚
â”‚  â”œâ”€ Ajout: hour = hour(timestamp)                           â”‚
â”‚  â””â”€ Ajout: day_of_week = dayofweek(timestamp)              â”‚
â”‚                                                              â”‚
â”‚  Ã‰TAPE 5: CatÃ©gorisation                                     â”‚
â”‚  â””â”€ status = CASE WHEN congestion < 30 THEN 'Fluide'       â”‚
â”‚               WHEN congestion < 60 THEN 'ModÃ©rÃ©'            â”‚
â”‚               ELSE 'CongestionnÃ©' END                        â”‚
â”‚                                                              â”‚
â”‚  Ã‰TAPE 6: AgrÃ©gation (Optionnel)                            â”‚
â”‚  â””â”€ groupBy(window(timestamp, "5 minutes"), zone_id)       â”‚
â”‚      .agg(avg(congestion), max(speed))                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LOAD (Chargement)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  DESTINATION 1: PostgreSQL (Hot Data)                       â”‚
â”‚  â”œâ”€ Format: Relationnel (tables)                            â”‚
â”‚  â”œâ”€ Mode: APPEND                                             â”‚
â”‚  â”œâ”€ FrÃ©quence: Micro-batch 30s                             â”‚
â”‚  â”œâ”€ RÃ©tention: < 1 heure (cleanup automatique)             â”‚
â”‚  â””â”€ Optimisation: Index B-Tree                              â”‚
â”‚                                                              â”‚
â”‚  DESTINATION 2: HDFS (Cold Data)                            â”‚
â”‚  â”œâ”€ Format: Parquet (compression Snappy)                    â”‚
â”‚  â”œâ”€ Mode: APPEND                                             â”‚
â”‚  â”œâ”€ Partitionnement: year/month/day                         â”‚
â”‚  â”œâ”€ RÃ©tention: IllimitÃ©e                                    â”‚
â”‚  â””â”€ Optimisation: Predicate pushdown                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Code ETL Complet
```python
# spark/streaming_docker.py

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION SPARK
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

spark = SparkSession.builder \
    .appName("TrafficETL") \
    .config("spark.sql.shuffle.partitions", "4") \
    .config("spark.streaming.kafka.maxRatePerPartition", "1000") \
    .getOrCreate()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXTRACT : Lecture Kafka
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

schema = StructType([
    StructField("zone_id", StringType()),
    StructField("zone_name", StringType()),
    StructField("latitude", FloatType()),
    StructField("longitude", FloatType()),
    StructField("current_speed", FloatType()),
    StructField("free_flow_speed", FloatType()),
    StructField("timestamp", StringType()),
    StructField("source", StringType())
])

df_kafka = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "traffic_raw") \
    .option("startingOffsets", "latest") \
    .load()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRANSFORM : Traitement
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. Parsing JSON
df_parsed = df_kafka.select(
    from_json(col("value").cast("string"), schema).alias("data")
).select("data.*")

# 2. Nettoyage
df_clean = df_parsed.filter(
    (col("current_speed") > 0) &
    (col("current_speed") <= 200) &
    (col("zone_id").isNotNull())
)

# 3. Calculs
df_calc = df_clean.withColumn(
    "congestion_level",
    when(col("free_flow_speed") > 0,
         ((col("free_flow_speed") - col("current_speed")) 
          / col("free_flow_speed")) * 100
    ).otherwise(0)
)

# 4. Enrichissement temporel
df_enriched = df_calc \
    .withColumn("timestamp_parsed", to_timestamp(col("timestamp"))) \
    .withColumn("year", year(col("timestamp_parsed"))) \
    .withColumn("month", month(col("timestamp_parsed"))) \
    .withColumn("day", dayofmonth(col("timestamp_parsed"))) \
    .withColumn("hour", hour(col("timestamp_parsed"))) \
    .withColumn("day_of_week", dayofweek(col("timestamp_parsed")))

# 5. CatÃ©gorisation
df_categorized = df_enriched.withColumn(
    "status",
    when(col("congestion_level") < 30, "Fluide")
    .when(col("congestion_level") < 60, "ModÃ©rÃ©")
    .otherwise("CongestionnÃ©")
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOAD : Ã‰criture Multi-Destinations
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# DESTINATION 1: PostgreSQL
def write_to_postgres(batch_df, batch_id):
    """Ã‰criture PostgreSQL avec gestion d'erreurs"""
    try:
        batch_df.select(
            "zone_id",
            "timestamp_parsed",
            "current_speed",
            "free_flow_speed",
            "congestion_level",
            "status"
        ).write \
            .format("jdbc") \
            .option("url", "jdbc:postgresql://postgres:5432/traffic") \
            .option("dbtable", "traffic_data") \
            .option("user", "postgres") \
            .option("password", "postgres") \
            .mode("append") \
            .save()
        
        print(f" Batch {batch_id} Ã©crit dans PostgreSQL")
    except Exception as e:
        print(f" Erreur batch {batch_id}: {str(e)}")

query_pg = df_categorized.writeStream \
    .foreachBatch(write_to_postgres) \
    .option("checkpointLocation", "/tmp/checkpoint/postgres") \
    .trigger(processingTime='30 seconds') \
    .start()

# DESTINATION 2: HDFS
query_hdfs = df_categorized.writeStream \
    .format("parquet") \
    .option("path", "hdfs://namenode:9000/traffic/clean") \
    .option("checkpointLocation", "/tmp/checkpoint/hdfs") \
    .partitionBy("year", "month", "day") \
    .trigger(processingTime='30 seconds') \
    .start()

# Attendre la terminaison
query_pg.awaitTermination()
query_hdfs.awaitTermination()
```

---

## Persistance des DonnÃ©es

### Architecture Multi-Niveaux
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  HOT DATA (PostgreSQL)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ RÃ©tention     : < 1 heure                                    â”‚
â”‚ Volume        : ~100 MB                                      â”‚
â”‚ Latence       : < 10 ms                                      â”‚
â”‚ Usage         : Dashboard temps rÃ©el, WebSocket             â”‚
â”‚ Optimisation  : Index B-Tree, Pool connexions              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  WARM DATA (Kafka)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ RÃ©tention     : 7 jours                                      â”‚
â”‚ Volume        : ~500 MB                                      â”‚
â”‚ Latence       : < 50 ms                                      â”‚
â”‚ Usage         : Buffer, Replay, Debug                       â”‚
â”‚ Optimisation  : Compression gzip, Segments                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  COLD DATA (HDFS)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ RÃ©tention     : IllimitÃ©e                                    â”‚
â”‚ Volume        : 500 MB â†’ 100 TB+                            â”‚
â”‚ Latence       : 2-5 secondes                                â”‚
â”‚ Usage         : Analytics, ML, Rapports                     â”‚
â”‚ Optimisation  : Parquet, Partitionnement, Compression      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### SÃ©paration DonnÃ©es/MÃ©tadonnÃ©es

#### PostgreSQL

**MÃ©tadonnÃ©es** (rapides, petites)
- `pg_catalog` : SchÃ©ma des tables
- `pg_stat` : Statistiques de requÃªtes
- Index B-Tree : Structure d'accÃ¨s rapide

**DonnÃ©es** (lentes, grandes)
- Heap files : Lignes de donnÃ©es
- TOAST : Gros objets (> 2KB)
```sql
-- Exemple : Utilisation d'index
SELECT * FROM traffic_data 
WHERE zone_id = 'champs_elysees' 
  AND timestamp > NOW() - INTERVAL '1 hour';

-- Sans index : Seq Scan (45 ms)
-- Avec index : Index Scan (0.3 ms) â†’ 150x plus rapide
```

#### HDFS

**MÃ©tadonnÃ©es** (NameNode)
- `fsimage` : Snapshot du namespace
- `edits` : Journal des modifications
- Mapping : Fichier â†’ Blocs â†’ DataNodes

**DonnÃ©es** (DataNode)
- Blocs de 64 MB
- RÃ©plication (typiquement x3)
- Checksum pour intÃ©gritÃ©
```
Fichier: part-00000.parquet (150 MB)
â”œâ”€ Bloc 1: 64 MB sur DataNode1, DataNode2
â”œâ”€ Bloc 2: 64 MB sur DataNode2, DataNode3
â””â”€ Bloc 3: 22 MB sur DataNode1, DataNode3
```

#### Parquet

**MÃ©tadonnÃ©es** (Footer)
- SchÃ©ma : Types et noms de colonnes
- Statistiques : Min/Max par colonne
- Row Groups : DÃ©coupage logique

**DonnÃ©es** (Row Groups)
- Colonnes compressÃ©es (Snappy/Gzip)
- Format binaire optimisÃ©
- Lecture sÃ©lective
```python
# Predicate Pushdown : Lecture seulement ce qui est nÃ©cessaire
df = spark.read.parquet("/traffic/clean")
result = df.filter(col("congestion_level") > 70) \
           .select("zone_id", "timestamp")

# Parquet lit :
# MÃ©tadonnÃ©es (footer) pour trouver Row Groups pertinents
# Colonnes "zone_id", "timestamp", "congestion_level" seulement
# Ignore toutes les autres colonnes
# RÃ©sultat : 90% de donnÃ©es non lues !
```

### Volumes Docker
```yaml
volumes:
  postgres_data:      # /var/lib/postgresql/data
  hadoop_namenode:    # /hadoop/dfs/name
  hadoop_datanode:    # /hadoop/dfs/data
```

**Commandes de gestion :**
```bash
# Lister les volumes
docker volume ls

# Inspecter un volume
docker volume inspect tp_traffic_routier_temps_reel_postgres_data

# Backup PostgreSQL
docker run --rm \
  -v tp_traffic_routier_temps_reel_postgres_data:/data \
  -v $(pwd):/backup \
  alpine tar czf /backup/postgres_backup.tar.gz /data

# Restore PostgreSQL
docker run --rm \
  -v tp_traffic_routier_temps_reel_postgres_data:/data \
  -v $(pwd):/backup \
  alpine tar xzf /backup/postgres_backup.tar.gz -C /

# Supprimer un volume ( Attention : perte de donnÃ©es)
docker volume rm tp_traffic_routier_temps_reel_postgres_data
```

---

##  Performance

### Benchmarks

| OpÃ©ration | Latence | Throughput | Notes |
|-----------|---------|------------|-------|
| **API REST (PostgreSQL)** | < 10 ms | 1000 req/s | Index B-Tree optimisÃ© |
| **WebSocket Update** | < 50 ms | Real-time | Push bidirectionnel |
| **Kafka Ingestion** | < 5 ms | 10,000 msg/s | Compression gzip |
| **Spark Micro-batch** | 30 s | 5000 records/batch | Configurable |
| **HDFS Write (Parquet)** | 1-2 s | 100 MB/s | Compression Snappy |
| **HDFS Read (Parquet)** | 2-5 s | 500 MB/s | Predicate pushdown |
| **PostgreSQL Insert** | < 1 ms | 10,000 inserts/s | Bulk insert |

### Optimisations ImplÃ©mentÃ©es

#### PostgreSQL
```sql
-- Index B-Tree sur colonnes frÃ©quemment utilisÃ©es
CREATE INDEX idx_traffic_timestamp ON traffic_data(timestamp);
CREATE INDEX idx_traffic_zone_id ON traffic_data(zone_id);
CREATE INDEX idx_traffic_zone_timestamp ON traffic_data(zone_id, timestamp);

-- Pool de connexions
pool_size=10
max_overflow=20

-- RÃ©sultat : RequÃªtes < 10 ms
```

#### HDFS
```python
# Partitionnement par date
df.write.partitionBy("year", "month", "day").parquet(...)

# Compression Parquet (80% rÃ©duction)
df.write.option("compression", "snappy").parquet(...)

# RÃ©sultat : 
# - 1 GB CSV â†’ 200 MB Parquet
# - Lecture sÃ©lective par date
```

#### Kafka
```python
# Compression messages
producer = KafkaProducer(
    compression_type='gzip',  # 50-70% rÃ©duction
    batch_size=16384,         # Batch 16 KB
    linger_ms=100             # Attendre 100ms pour remplir batch
)

# RÃ©sultat : DÃ©bit 10,000+ msg/s
```

#### React Frontend
```javascript
// Ã‰viter re-renders inutiles
const MemoizedMap = React.memo(Map);

// Debounce sur filtres
const debouncedFilter = useDebouncedCallback(
    (value) => setFilter(value),
    300
);

// RÃ©sultat : UI fluide mÃªme avec 1000+ updates/min
```

### ScalabilitÃ© Horizontale

#### Kafka
```yaml
# Augmenter partitions
kafka-topics --alter --topic traffic_raw --partitions 10

# Ajouter brokers
docker-compose up -d --scale kafka=3

# RÃ©sultat : DÃ©bit linÃ©aire (10x partitions = 10x dÃ©bit)
```

#### Spark
```yaml
# Ajouter workers
docker-compose up -d --scale spark-worker=5

# Ajuster parallelisme
spark.sql.shuffle.partitions=20

# RÃ©sultat : Traitement 5x plus rapide
```

#### HDFS
```yaml
# Ajouter DataNodes
docker-compose up -d --scale datanode=5

# Augmenter rÃ©plication
hdfs dfs -setrep -R 3 /traffic

# RÃ©sultat : 
# - CapacitÃ© 5x plus grande
# - Lecture parallÃ¨le plus rapide
```

---

##  Tests

### Tests Backend

#### Installation
```bash
cd backend
pip install pytest pytest-asyncio pytest-cov httpx
```

#### ExÃ©cution
```bash
# Tous les tests
pytest tests/ -v

# Tests spÃ©cifiques
pytest tests/test_api.py -v

# Avec couverture
pytest --cov=app tests/

# GÃ©nÃ©rer rapport HTML
pytest --cov=app --cov-report=html tests/
open htmlcov/index.html
```

#### Exemple de Test
```python
# backend/tests/test_api.py

import pytest
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

def test_get_live_zones():
    """Test rÃ©cupÃ©ration zones en temps rÃ©el"""
    response = client.get("/api/zones/live")
    
    assert response.status_code == 200
    assert isinstance(response.json(), list)
    assert len(response.json()) > 0
    
    zone = response.json()[0]
    assert "id" in zone
    assert "name" in zone
    assert "congestion_level" in zone

def test_get_weekly_data():
    """Test rÃ©cupÃ©ration donnÃ©es hebdomadaires"""
    response = client.get("/api/zones/weekly-data?days=7")
    
    assert response.status_code == 200
    data = response.json()
    assert isinstance(data, list)

def test_hdfs_stats():
    """Test statistiques HDFS"""
    response = client.get("/api/zones/hdfs-stats")
    
    assert response.status_code == 200
    stats = response.json()
    assert "available" in stats
```

### Tests Frontend

#### Installation
```bash
cd frontend
npm install -D vitest @testing-library/react @testing-library/jest-dom
```

#### ExÃ©cution
```bash
# Tous les tests
npm run test

# Mode watch
npm run test:watch

# Avec UI
npm run test:ui
```

#### Exemple de Test
```javascript
// frontend/src/components/__tests__/Dashboard.test.jsx

import { describe, it, expect } from 'vitest';
import { render, screen } from '@testing-library/react';
import Dashboard from '../Dashboard/Dashboard';

describe('Dashboard', () => {
    it('should render KPI cards', () => {
        const mockZones = [
            { id: '1', name: 'Zone 1', congestion_level: 45 }
        ];
        const mockStats = { averageCongestion: 45 };
        
        render(<Dashboard zones={mockZones} stats={mockStats} />);
        
        expect(screen.getByText('Congestion Moyenne')).toBeInTheDocument();
        expect(screen.getByText('45%')).toBeInTheDocument();
    });
});
```

### Tests d'IntÃ©gration
```bash
#!/bin/bash
# scripts/test_pipeline.sh

echo " Test du pipeline complet"

# 1. Envoyer message dans Kafka
echo "1ï¸ Envoi message Kafka..."
docker-compose exec kafka kafka-console-producer \
  --bootstrap-server localhost:9092 \
  --topic traffic_raw << EOF
{"zone_id":"test","current_speed":50,"timestamp":"2025-11-21T10:00:00Z"}
EOF

# 2. Attendre traitement (30s)
echo "2ï¸ Attente traitement Spark..."
sleep 35

# 3. VÃ©rifier PostgreSQL
echo "3ï¸ VÃ©rification PostgreSQL..."
docker-compose exec postgres psql -U postgres -d traffic -c \
  "SELECT COUNT(*) FROM traffic_data WHERE zone_id='test';"

# 4. VÃ©rifier HDFS
echo "4ï¸ VÃ©rification HDFS..."
docker exec namenode hadoop fs -ls /traffic/clean/year=2025/

# 5. VÃ©rifier API
echo "5ï¸ VÃ©rification API..."
curl -s http://localhost:8000/api/zones/live | jq

echo " Tests terminÃ©s"
```

---

##  Documentation API

### OpenAPI/Swagger

AccÃ©dez Ã  **http://localhost:8000/docs** pour la documentation interactive complÃ¨te.

### Endpoints

#### Zones

| MÃ©thode | Endpoint | Description | ParamÃ¨tres |
|---------|----------|-------------|------------|
| `GET` | `/api/zones/live` | Zones temps rÃ©el | - |
| `GET` | `/api/zones/weekly-data` | DonnÃ©es hebdo (HDFS) | `days` (int) |
| `GET` | `/api/zones/hourly-distribution` | Distribution horaire | `days` (int) |
| `GET` | `/api/zones/hdfs-stats` | Stats HDFS | - |

#### AgrÃ©gats

| MÃ©thode | Endpoint | Description | ParamÃ¨tres |
|---------|----------|-------------|------------|
| `GET` | `/api/aggregates/stats` | Stats agrÃ©gÃ©es | - |

#### WebSocket

| Protocole | Endpoint | Description | Format |
|-----------|----------|-------------|--------|
| `WS` | `/ws/traffic` | Flux temps rÃ©el | JSON |

### SchÃ©mas de DonnÃ©es

#### Zone
```json
{
  "id": "string",
  "name": "string",
  "location": "string",
  "latitude": "float",
  "longitude": "float",
  "current_speed": "float",
  "free_flow_speed": "float",
  "congestion_level": "float",
  "status": "string",
  "vehicles": "int"
}
```

#### WeeklyData
```json
{
  "date": "string (YYYY-MM-DD)",
  "congestion": "float",
  "speed": "float",
  "measures": "int"
}
```

#### HourlyDistribution
```json
{
  "hour": "string (HHh)",
  "congestion": "float"
}
```

---

##  DÃ©veloppement

### Configuration Environnement Local

#### Backend
```bash
# CrÃ©er environnement virtuel
cd backend
python -m venv venv
source venv/bin/activate  # Linux/macOS
# ou
venv\Scripts\activate  # Windows

# Installer dÃ©pendances
pip install -r requirements.txt

# Variables d'environnement
export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/traffic
export KAFKA_BOOTSTRAP_SERVERS=localhost:9092

# Lancer serveur dÃ©veloppement
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

#### Frontend
```bash
# Installer dÃ©pendances
cd frontend
npm install

# Variables d'environnement
echo "VITE_API_URL=http://localhost:8000/api" > .env.local

# Lancer serveur dÃ©veloppement
npm run dev

# Build production
npm run build

# Preview build
npm run preview
```

### Hot Reload

**Backend FastAPI** : Utilise `--reload` pour rechargement automatique

**Frontend Vite** : HMR (Hot Module Replacement) natif

### Debugging

#### Backend (VS Code)
```json
// .vscode/launch.json
{
  "version": "0.2.0",
  "configurations": [
    {
      "name": "Python: FastAPI",
      "type": "python",
      "request": "launch",
      "module": "uvicorn",
      "args": [
        "app.main:app",
        "--reload",
        "--host", "0.0.0.0",
        "--port", "8000"
      ],
      "jinja": true,
      "justMyCode": false
    }
  ]
}
```

#### Frontend (Chrome DevTools)
```
1. Ouvrir http://localhost:5173
2. F12 â†’ Sources â†’ Vos fichiers sont mappÃ©s
3. Breakpoints fonctionnent directement
```

---

## Troubleshooting

### ProblÃ¨me : Backend ne peut pas se connecter Ã  HDFS
```bash
# SymptÃ´me
Erreur lecture agrÃ©gats : java.net.UnknownHostException: namenode

# Diagnostic
docker network inspect tp_traffic_routier_temps_reel_traffic-network | grep namenode

# Si namenode absent du rÃ©seau :
# Solution : Ajouter networks dans docker-compose.yml
namenode:
  networks:
    - traffic-network  # â† Ajouter cette ligne
```

### ProblÃ¨me : DonnÃ©es HDFS non visibles
```bash
# VÃ©rifier HDFS
docker exec namenode hadoop fs -ls /traffic/clean

# Si vide : Charger donnÃ©es
docker-compose exec backend python3 csv_to_hdfs_docker.py --days 30

# VÃ©rifier WebUI HDFS
open http://localhost:9870
```

### ProblÃ¨me : Frontend ne se connecte pas au Backend
```bash
# VÃ©rifier CORS
curl -H "Origin: http://localhost:5173" \
     -H "Access-Control-Request-Method: GET" \
     -X OPTIONS http://localhost:8000/api/zones/live

# Devrait retourner :
# Access-Control-Allow-Origin: http://localhost:5173

# Si erreur : VÃ©rifier backend/app/main.py
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

### ProblÃ¨me : Kafka ne dÃ©marre pas
```bash
# VÃ©rifier Zookeeper
docker-compose ps zookeeper

# Si unhealthy : VÃ©rifier logs
docker-compose logs zookeeper | tail -50

# Solution : RedÃ©marrer services
docker-compose restart zookeeper
sleep 10
docker-compose restart kafka
```

### ProblÃ¨me : PostgreSQL "role does not exist"
```bash
# RecrÃ©er la base
docker-compose down -v
docker-compose up -d postgres

# Attendre initialisation
sleep 10

# VÃ©rifier
docker-compose exec postgres psql -U postgres -c "\l"
```

### ProblÃ¨me : ENOSPC (No space left)
```bash
# VÃ©rifier espace disque
df -h

# Nettoyer Docker
docker system prune -a --volumes

```


##  Auteurs

- **[Eva Depaepe]**
- **[Emilie Delrue]** 

---

## Remerciements

### APIs & Services
- **[TomTom](https://developer.tomtom.com/)** pour l'API Traffic Flow

### Technologies Open Source
- **[Apache Software Foundation](https://apache.org/)** pour Kafka, Spark, Hadoop
- **[FastAPI](https://fastapi.tiangolo.com/)** par SebastiÃ¡n RamÃ­rez
- **[React](https://react.dev/)** par Meta
- **[Leaflet](https://leafletjs.com/)** par Vladimir Agafonkin
- **[Tailwind CSS](https://tailwindcss.com/)** par Adam Wathan

---

##  Roadmap

### v1.1 (Court terme)

- [ ] Authentification JWT
- [ ] Alertes email/SMS (congestion > seuil)
- [ ] Export PDF des rapports
- [ ] Historique de recherche
- [ ] Mode sombre (dark theme)

### v1.2 (Moyen terme)

- [ ] Machine Learning : PrÃ©diction trafic
- [ ] IntÃ©gration Grafana/Prometheus
- [ ] API GraphQL
- [ ] Multi-langues (i18n)
- [ ] Application mobile (React Native)

### v2.0 (Long terme)

- [ ] Temps rÃ©el < 1s (Kafka Streams)
- [ ] Architecture microservices
- [ ] DÃ©ploiement Kubernetes
- [ ] CI/CD complet (GitHub Actions)
- [ ] Monitoring avancÃ© (ELK Stack)

---

docker-compose up -d --build
docker-compose exec backend python3 -c "from app.db.database import engine; from app.db.models import Base; Base.metadata.create_all(bind=engine)"
docker-compose exec backend python3 -c "from app.db.database import engine; from app.db.models import Base; Base.metadata.create_all(bind=engine)"
docker-compose exec backend bash -c "export PYTHONPATH=/app && python3 -u -m app.services.kafka_consumer"
docker-compose exec backend bash -c "export PYTHONPATH=/app && python3 -u -m app.services.kafka_producer"